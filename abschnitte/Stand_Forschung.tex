\section{Stand der Forschnung}

Das Testen von Software dient vor allem der Vermeidung von Fehlern innerhalb des Programmablaufs. Man unterscheidet dabei unter anderem verschiedene Testarten und verschiedene Testmethodiken, von denen einige im Folgenden näher erläutert werden.

\subsection{Testarten}

Es gibt verschiedene Arten von Softwaretests. Diese unterscheiden sich vor allem darin, was getestet wird, aber auch darin, wer den Test durchführt. Neben den Modul- und Funktionstests, die vom Entwicklungsteam durchgeführt werden, gibt es noch System- und Abnahmetests, bei denen das gesamte Softwareprodukt vom Kunden bzw. späteren Benutzer entweder auf einem Testsystem oder auf dem Produktivsystem getestet wird.

\subsubsection{Modultests}

Modultests (im Englischen auch Unit Tests genannt) sind Tests, die einzelne Module der Software testen. Für gewöhnlich versucht man hier möglichst kleine Module zu testen, um die Stabilität der einzelnen Tests zu steigern. Es wird also vor allem eine einzelne Methode bzw. Funktion getestet. Da der Test somit direkt mit einem Programmcode-Stück interagiert, ist klar, dass er selbst in der gleichen Programmiersprache verfasst wird. Er wird somit von einem Entwickler geschrieben. Der Test sollte sich dabei auf das gewünschte Verhalten der Methode konzentrieren und nicht auf deren konkrete Implementierung abstellen (Design-by-contract). Bei den Modultests hat sich im Laufe der Zeit eine bestimmte Definitionsweise etabliert, der die meisten Modultest-Frameworks folgen. Es gibt sogar eine Reihe von Frameworks (xUnit), die von Sprache zu Sprache portiert werden und alle auf das von Kent Beck entworfene SUnit für die Sprache Smalltalk zurückzuführen sind. Von ihm stammt auch die erste Portierung in die Sprache Java namens JUnit. Typisch für diese Frameworks ist, dass die eigentlichen Tests ihrerseits Methoden innerhalb einer Testklasse (Test-Case) sind. Mehrere dieser Testklassen können dann in Gruppen organisiert werden (Test-Suite). Außerdem gibt es fast immer die Möglichkeit, innerhalb einer Testklasse Methoden für die Initialisierung bzw. Deinitialsierung der Testumgebung zu definieren, die vor und nach jeder einzelnen Testmethode ausgeführt weren (Set-Up- und Tear-Down-Methoden). Innerhalb der eigentlichen Testmethode stehen dann weitere Hilfsmethoden zur Verfügung, mit denen einfach Behauptungen über das Testsubjekt formuliert werden können (Assertions), die dann während der eigentlichen Testausführung überprüft werden. Ein Modultest-Framework bietet neben einer einfachen Test-Definitions-Syntax auch ein Programm, Test-Runner genannt, welches die Auswahl bestimmter Tests erlaubt und nach deren Ausführung ein Testergebnis darstellt.

\subsubsection{Funktionstests}

Der Funktionstest dient dazu, das Zusammenspiel mehrerer Programmbausteine zu testen. Gerne wird er auch als Schnittstellentest bezeichnet, da er gegen eine bestimmte Schnittstelle des Programms arbeitet. Bei Webanwendungen bietet es sich hierbei an, die HTTP-Schnittstelle der Anwendung zu testen. Man spezifiziert also Aufrufe an den Webserver und überprüft dabei, ob die Antwort bestimmten Erwartungen entspricht. Meist werden auch die Funktionstest vom Entwickler selbst geschrieben und damit auch gerne wieder in der gleichen Programmiersprache wie das zu testende Programm selbst.

\subsubsection{System- und Abnahmetests}

Da bei den System- und Abnahmetests das fertige Softwareprodukt durch den Kunden bzw. späteren Benutzer getestet wird, ist für die Implementierung dieser Tests die der Software zugrunde liegende Programmiersprache unerheblich. Es gibt hier vor allem keine sprachspezifischen Frameworks.

\subsection{Testmethodiken}

Testmethodiken dienen dazu, das Schreiben der Tests in den eigentlichen Software-Entwicklungsprozess zu integrieren. In den meisten Softwareprojekten haben sich dabei vor allem die folgenden Methodiken etabliert: Das \ac{TDD} (zu Deutsch "Testgetriebene Entwicklung"), das \ac{BDD} (zu Deutsch "Verhaltensgetriebene Entwicklung") und die \ac{CI} (zu Deutsch "Kontinuierliche Integration").

\subsubsection{Test Driven Development}

Beim \ac{TDD} handelt es sich um ein Vorgehen, dass der agilen Softwareentwicklung zugeschrieben wird. Im Gegensatz zur klassischen Softwareentwicklung, bei der Tests bei oder sogar erst nach der fertigen Implementierung des Programmcodes geschrieben werden, kann man vereinfacht sagen, dass der Entwickler beim \ac{TDD} erst den Test und dann den eigentlichen Programmcode schreibt. Dies soll folgende Vorteile mit sich bringen:

\begin{itemize}
  \item Der Ansatz garantiert eine sehr hohe Testabdeckung.
  \item Es wird verhindert, dass nicht testbarer Code entsteht.
  \item Zu Projektende werden Ressourcen (Zeit, Budget) typischerweise knapp und verhindert oft die Fertigstellung nachträglicher Tests.
  \item Das vorherige Schreiben des Tests begünstigt, dass der Tests auf das gewünschte Verhalten abstellt, da die Implementierung ja noch nicht vorliegt (Design-by-contract).
\end{itemize}

Da der Entwickler die Tests bei diesem Vorgehen sehr oft ausführt, nämlich vor der Implementierung und während der Implementierung so oft, bis der Test bestanden ist, ist es sehr wichtig, dass das Ausführen der Tests sehr einfach vorgenommen werden kann. Man spricht gerne davon, dass die Tests "auf Knopfdruck" durchgeführt werden können müssen. Ein gutes Test-Framework bietet hierfür eine Möglichkeit, den Test-Runner direkt aus der \ac{IDE} des Entwicklers aufzurufen.

\subsubsection{Behavior Driven Development}

Die Idee des \ac{BDD} ist es, die Stabilität und Wartbarkeit von Testdefinitionen zu verbessern. Dabei werden die Tests oft als Prosa-Text verfasst, der keine technischen Details der Testimplementierung enthält. Dieser muss entsprechend einer sehr eingeschränkten Syntax, einer so genannten \ac{DSL}, folgen, da er zum Ausführen des Tests in entsprechenden Programmcode übersetzt wird. Weiterer Vorteil dieses Ansatzes ist es, dass die Tests auch von Nicht-Entwicklern (z.B. dem Kunden) besser gelesen und verstanden werden können.

\subsubsection{Continous Integration}

Bei der \ac{CI} geht es darum, die Test-Ergebnisse und andere Metriken teamübergreifend zu erfassen und kontinuierlich zu verfolgen, um so im Lauf der Zeit positive oder negative Tendenzen in der Entwicklung des Softwareprodukts erkennen zu können. Typischerweise werden die Testergebnisse und andere Metriken in einem sogenannten \ac{CI}-Server archiviert, der sich darum kümmert, diese von Zeit zu Zeit abzufragen bzw. selbständig zu generieren. Oft tut der Server dies zum Beispiel jede Nacht (Nightly Builds). Viele solcher CI-Server bieten aber sogar die Möglichkeit, sie über jede neue Version der Software zu informieren, die in der Versionskontrolle abgelegt wird und daraufhin ein neues Ausführen der Tests usw. auszulösen. In Bezug auf die Tests ist es dazu notwendig, dass diese nicht durch einen Entwickler sondern programmatisch ausgeführt werden können und ihre Ergebnisse anschließend auf maschinenlesbare Art und Weise zur Verfügung stellen.